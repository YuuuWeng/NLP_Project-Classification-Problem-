{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = { \"in_reply_to_status_id\": None}\n",
    "if a[\"in_reply_to_status_id\"]!=None:\n",
    "    print (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def twitter_feature_selection(json_file):\n",
    "    selected_feature = {}\n",
    "    selected_feature[\"text\"] = json_file[\"text\"]\n",
    "    selected_feature[\"id\"] = json_file[\"id\"]\n",
    "    selected_feature[\"created_at\"] = json_file[\"created_at\"]\n",
    "    if \"referenced_tweets\" in json_file.keys():   \n",
    "        selected_feature[\"referenced_tweets\"] = json_file[\"referenced_tweets\"][0][\"id\"]\n",
    "    selected_feature['retweet_count'] = json_file['public_metrics']['retweet_count']\n",
    "    \n",
    "    # selected_feature[\"hashtag\"] = re.search(r'(\\w+)+', json_file[\"text\"])\n",
    "    return selected_feature\n",
    "\n",
    "def test_feature_selection(json_file):\n",
    "    selected_feature = {}\n",
    "    selected_feature[\"text\"] = json_file[\"text\"]\n",
    "    selected_feature[\"created_at\"] = json_file[\"created_at\"]\n",
    "    if json_file[\"in_reply_to_status_id_str\"]!= None:   \n",
    "        selected_feature[\"referenced_tweets\"] = json_file[\"in_reply_to_status_id_str\"]\n",
    "    selected_feature[\"id\"] = json_file[\"id_str\"]\n",
    "    selected_feature['retweet_count'] = json_file['retweet_count']\n",
    "\n",
    "    return selected_feature\n",
    "\n",
    "def convert_label(label):\n",
    "    if label == \"rumour\":\n",
    "        return 1\n",
    "    elif label == \"nonrumour\":\n",
    "        return 0 \n",
    "def merge_dictionary_list(dict_list):\n",
    "      return {\n",
    "    k: [d.get(k) for d in dict_list if k in d]\n",
    "    for k in set().union(*dict_list)\n",
    "  } \n",
    "def remove_unwanted_keys(dict_list, keys):\n",
    "    for item in dict_list:\n",
    "        item.pop(keys, None)\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getgraph(events):\n",
    "    graph = {}\n",
    "    for e in events:   \n",
    "        graph[e[\"id\"]]=[]\n",
    "\n",
    "    for e in events:\n",
    "        if \"referenced_tweets\" in e.keys():\n",
    "            #str\n",
    "            k = e[\"referenced_tweets\"]\n",
    "            v = e[\"id\"]\n",
    "            if k in graph.keys(): \n",
    "                graph[k].append(e)\n",
    "            else:\n",
    "                graph[k] = [e]\n",
    "    return(graph)\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# time_sequence = []\n",
    "\n",
    "# ## sort dfs as the reply order\n",
    "# ##everynode type is (id,text,time,)\n",
    "# def dfs(visited, graph, node):\n",
    "    \n",
    "#     if node[\"id\"] not in visited:\n",
    "#         time_sequence.append(node)\n",
    "# #         print (node)\n",
    "#         visited.add(node[\"id\"])\n",
    "#         for neighbour in graph[node[\"id\"]]:\n",
    "            \n",
    "#             dfs(visited, graph, neighbour)\n",
    "\n",
    "# eventg= getgraph(testevent)\n",
    "# # # Driver Code\n",
    "# dfs(visited, eventg, sourcetweet)\n",
    "# print (time_sequence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## sort dfs as the reply order\n",
    "##everynode type is (id,text,time,)\n",
    "def dfs(visited, graph, node,time_sequence):\n",
    "    \n",
    "    if node[\"id\"] not in visited:\n",
    "        time_sequence.append(node)\n",
    "#         print (node)\n",
    "        visited.add(node[\"id\"])\n",
    "        for neighbour in graph[node[\"id\"]]:\n",
    "            \n",
    "            dfs(visited, graph, neighbour,time_sequence)\n",
    "    return time_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    if (file_name == 'train' or file_name == 'dev'):\n",
    "        total_json = []\n",
    "        finished_dict = []\n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".data.txt\", \"r\") as f:\n",
    "            file_name_event = f.readlines()      \n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".label.txt\", \"r\") as f:\n",
    "            file_name_label = f.readlines()\n",
    "        for event_str, event_label in zip(file_name_event, file_name_label):\n",
    "            event_list = event_str.rstrip().split(\",\")\n",
    "            event_label = event_label.rstrip().split(\",\")\n",
    "            # print(type(event_label))\n",
    "            event_json = []\n",
    "            path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + event_list[0] + \".json\"\n",
    "            # print(path)\n",
    "            # print(os.path.exists(path))\n",
    "            if os.path.exists(path):\n",
    "                for id in event_list:\n",
    "                    path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + id + \".json\"\n",
    "                    # print(path)\n",
    "                    # print(os.path.exists(path))\n",
    "                    if os.path.exists(path):\n",
    "                        related_tweet = json.load(open(path, \"r\"))\n",
    "                        # print(related_tweet)\n",
    "                        event_json.append(twitter_feature_selection(related_tweet))\n",
    "                \n",
    "                event_json = sorted(event_json, key=lambda x: time.mktime(time.strptime(x[\"created_at\"], '%Y-%m-%dT%H:%M:%S.%fZ')))\n",
    "                sourcetweet = event_json[0]\n",
    "                eventgraph = getgraph(event_json)\n",
    "                visited = set()\n",
    "                sortresult = []\n",
    "                event_json = dfs(visited, eventgraph, sourcetweet,sortresult)\n",
    "                \n",
    "                event_json.append({\"label\":convert_label(''.join(event_label))})\n",
    "                # print(event_json)\n",
    "                merged_dict = merge_dictionary_list(event_json)\n",
    "                # print(merged_dict)\n",
    "                total_json.append(merged_dict)\n",
    "                # print(total_json)\n",
    "                finished_dict = remove_unwanted_keys(total_json, \"created_at\")\n",
    "    elif (file_name == 'test'):\n",
    "        total_json = []\n",
    "        finished_dict = []\n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".data.txt\", \"r\") as f:\n",
    "            file_name_event = f.readlines()      \n",
    "        for event_str in file_name_event:\n",
    "            event_list = event_str.rstrip().split(\",\")\n",
    "            event_json = []\n",
    "            path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + event_list[0] + \".json\"\n",
    "            # print(path)\n",
    "            # print(os.path.exists(path))\n",
    "            if os.path.exists(path):\n",
    "                for id in event_list:\n",
    "                    path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + id + \".json\"\n",
    "                    # print(path)\n",
    "                    # print(os.path.exists(path))\n",
    "                    if os.path.exists(path):\n",
    "                        related_tweet = json.load(open(path, \"r\"))\n",
    "                        # print(related_tweet)\n",
    "                        event_json.append(test_feature_selection(related_tweet))\n",
    "                event_json = sorted(event_json, key=lambda x: time.mktime(time.strptime(x[\"created_at\"], '%a %b %d %H:%M:%S %z %Y')))\n",
    "                # print(event_json)\n",
    "                sourcetweet = event_json[0]\n",
    "                eventgraph = getgraph(event_json)\n",
    "                visited = set()\n",
    "                sortresult = []\n",
    "                event_json = dfs(visited, eventgraph, sourcetweet,sortresult)\n",
    "                \n",
    "                merged_dict = merge_dictionary_list(event_json)\n",
    "                # print(merged_dict)\n",
    "                total_json.append(merged_dict)\n",
    "                # print(total_json)\n",
    "                finished_dict = remove_unwanted_keys(total_json, \"created_at\")\n",
    "    return pd.DataFrame(finished_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Required data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data(\"train\")\n",
    "df_dev = load_data(\"dev\")\n",
    "df_test = load_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(tweet):\n",
    "    r1 = r'https?:/\\/\\S+'\n",
    "    ## remove @people\n",
    "    r2 = r'@[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…？“”‘’！\\\\]+'\n",
    "    ##remove tag\n",
    "    r3 = r'#[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…？“”‘’！\\\\]+'\n",
    "\n",
    "    tweet = tweet.encode('utf-8', 'replace').decode('utf-8')\n",
    "    tweet = re.sub(r1, \"\", tweet)\n",
    "    tweet = re.sub(r2, \"\", tweet)\n",
    "    tweet = re.sub(r3, \"\", str(tweet).lower().strip())\n",
    "    return tweet\n",
    "def give_emoji_free_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\" \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  \n",
    "        u\"\\U0001F1E6-\\U0001F1FF\" \n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the Text together and do the data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋',\n",
       " '@snooki 😩😫😩😫😩 they just keep finding stuff',\n",
       " '@snooki not fallin for that again',\n",
       " '@snooki nasty!!!!!! 😭',\n",
       " '@snooki no reason it should even exist 😱😱😱😱😱😩😩',\n",
       " '@snooki ok',\n",
       " '@snooki NOOOOO 😷',\n",
       " '@snooki WTF A PUPPY SIZED SPIDER',\n",
       " 'THIS IS THE SPIDER \\n\\n“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋”',\n",
       " \"@snooki NO NO NO NO NO NO! That's a horror movie spider!\",\n",
       " \"@snooki lol! I'm the same way with rats....\",\n",
       " '@snooki I thought it was going to be a bird😭😭😭😭',\n",
       " '@snooki BYEEEEE 👋👋👋👋👋👋',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” I JUST CRINGED!!!',\n",
       " '\"@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋\" why would you do this to me?',\n",
       " \"@snooki hence why I'm arachnophobic!!!!\",\n",
       " '@snooki @kltoleyX ⬆️..',\n",
       " '@snooki 🙈😷😷😷😷',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” @BrookieSnooki10',\n",
       " '@Merritt_Bwn ✋✋✋✋✋✋✋✋ FUCK THAT SHIT!!',\n",
       " '@snooki Gurllll. I got one for you. Google Camel Spider.',\n",
       " '@snooki 😭😭😭😭😭 OMG ah',\n",
       " '@snooki I hate you...  😱😭😷 my curiosity got the best of me',\n",
       " '@snooki hahahaha 😂',\n",
       " '@snooki Fuck u why would u make me😭',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋”😭😭😭😭',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” OMG😷😷😷😷',\n",
       " '@snooki I noped that so hard! 🙅',\n",
       " '@snooki I threw my phone across the room',\n",
       " '@snooki fuck you omg lol',\n",
       " \"@snooki holy balls!!! That's a BIG ass spider 😰🙊🙈🙈🙈\",\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋”😩😩 http://t.co/o90M9lD37u',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” @jarodjnellly do it',\n",
       " '@snooki wherever it lives just got added to my never want to visit list 🙅',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” @ViancaVMartinez @armandomtz1211',\n",
       " '@snooki ewwwwwwwww!',\n",
       " \"@snooki I'm scared of spiders the size of a dime 😭😭 #nogodno #traumatized\",\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” That is a fuck no',\n",
       " '@snooki (Good God Almighty)!',\n",
       " '@snooki you should try Google Camel Spider',\n",
       " '@snooki WHY WHY WWWHHHYYY WOULD U DO THAT TO US???',\n",
       " '@snooki I think I just pissed my pants...',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” WHAT THE ACTUAL 😳😳😷',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” if I ever saw one Id just go ahead &amp; die',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋”',\n",
       " '@snooki I hate you for that!!!! 💩💩💩💩😱😱😱😱👎👎👎👎',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” JODA NOOOO TIRE EL CELULAR😭😭😭😭😥',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋”@GraceBowlby',\n",
       " '@snooki 😷😷',\n",
       " '@snooki all aboard the NOPE train! 🚝',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” 😭😭😭😩😩😳',\n",
       " \"@snooki I wanna do it but I'm scared!\",\n",
       " '@snooki EW!!!',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” BYE',\n",
       " '@snooki Omg no',\n",
       " '@snooki thanks for that. 😞',\n",
       " \"“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” I'd rather die than see that\",\n",
       " '@snooki holy shit. Ew. No no no no no no no no no no.',\n",
       " '@snooki @JessieBFF4EVA annnnnd I now need to change the bed... #HugeAssSpider #PuppySize #AteThePuppy #BedShittage',\n",
       " '@snooki omg I literally stopped breathing!! 🙅🙅🙅🙅',\n",
       " '@snooki ...ewweww....no way',\n",
       " \"@snooki Did you watch the video on YouTube? It's really interesting.\",\n",
       " '@snooki I think you will do great,bc you and Jenny are great in front of the camera..Good luck',\n",
       " '@snooki EW! Why would you do that?! Shame on you 😷',\n",
       " '@snooki I kinda do what ever you tell me sooo....... That was not pretty 😅',\n",
       " '@snooki omggg I literally have goosebumps from googling that ugly thing!',\n",
       " '“@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOO✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋✋” @jahviaskristy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_train['text'] = df_train['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_train['text'] = df_train['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_train['label'] = df_train.apply(lambda x:x[\"label\"][0],axis=1)\n",
    "df_train['label'] = df_train['label'].astype('int')\n",
    "df_train['retweet_count'] = df_train.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "df_dev['text'] = df_dev.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_dev['text'] = df_dev['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_dev['text'] = df_dev['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_dev['label'] = df_dev.apply(lambda x:x[\"label\"][0],axis=1)\n",
    "df_dev['label'] = df_dev['label'].astype('int')\n",
    "df_dev['retweet_count'] = df_dev.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "\n",
    "df_test['text'] = df_test.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_test['text'] = df_test['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_test['text'] = df_test['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_test['retweet_count'] = df_test.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_train.drop(['id','referenced_tweets'],axis=1,inplace=True)\n",
    "df_dev.drop(['id','referenced_tweets'],axis=1,inplace=True)\n",
    "df_test.drop(['id','referenced_tweets'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_html(x))\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_html(x))\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_html(x))\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "df_train.dropna(axis=0, how='any', inplace=True)\n",
    "df_dev.dropna(axis=0, how='any', inplace=True)\n",
    "df_test.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google south american goliath birdeater. helllll noooooo  they just keep finding stuff not fallin for that again nasty!!!!!!  no reason it should even exist  ok nooooo  wtf a puppy sized spiderthis is the spider “ google south american goliath birdeater. helllll noooooo” no no no no no no! that\\'s a horror movie spider! lol! i\\'m the same way with rats.... i thought it was going to be a bird byeeeee “ google south american goliath birdeater. helllll noooooo” i just cringed!!!\" google south american goliath birdeater. helllll noooooo\" why would you do this to me? hence why i\\'m arachnophobic!!!!  .. “ google south american goliath birdeater. helllll noooooo” _bwn  fuck that shit!! gurllll. i got one for you. google camel spider.  omg ah i hate you...   my curiosity got the best of me hahahaha  fuck u why would u make me“ google south american goliath birdeater. helllll noooooo”“ google south american goliath birdeater. helllll noooooo” omg i noped that so hard!  i threw my phone across the room fuck you omg lol holy balls!!! that\\'s a big ass spider “ google south american goliath birdeater. helllll noooooo”  google south american goliath birdeater. helllll noooooo”  do it wherever it lives just got added to my never want to visit list “ google south american goliath birdeater. helllll noooooo”   ewwwwwwwww! i\\'m scared of spiders the size of a dime    google south american goliath birdeater. helllll noooooo” that is a fuck no (good god almighty)! you should try google camel spider why why wwwhhhyyy would u do that to us??? i think i just pissed my pants...“ google south american goliath birdeater. helllll noooooo” what the actual “ google south american goliath birdeater. helllll noooooo” if i ever saw one id just go ahead &amp; die“ google south american goliath birdeater. helllll noooooo” i hate you for that!!!! “ google south american goliath birdeater. helllll noooooo” joda noooo tire el celular“ google south american goliath birdeater. helllll noooooo”  all aboard the nope train! “ google south american goliath birdeater. helllll noooooo”  i wanna do it but i\\'m scared! ew!!!“ google south american goliath birdeater. helllll noooooo” bye omg no thanks for that. “ google south american goliath birdeater. helllll noooooo” i\\'d rather die than see that holy shit. ew. no no no no no no no no no no.  annnnnd i now need to change the bed...     omg i literally stopped breathing!!  ...ewweww....no way did you watch the video on youtube? it\\'s really interesting. i think you will do great,bc you and jenny are great in front of the camera..good luck ew! why would you do that?! shame on you  i kinda do what ever you tell me sooo....... that was not pretty  omggg i literally have goosebumps from googling that ugly thing!“ google south american goliath birdeater. helllll noooooo”'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweet_count    False\n",
       "text             False\n",
       "label            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4. can eating garlic help prevent infection wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185</td>\n",
       "      <td>french police chief killed himself after  atta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus disease (covid-19) advice for the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>ottawa police confirm that there were multiple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>if the primary focus of a government isn't to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0</td>\n",
       "      <td>4. it cannot be transmitted through goods manu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>83</td>\n",
       "      <td>desperate ted cruz claims planned parenthood s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>109</td>\n",
       "      <td>\"thoughts and prayers are not enough.\" pres. o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>177</td>\n",
       "      <td>police have surrounded this building where the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>4</td>\n",
       "      <td>excellent.  tasha and i just little giggle at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      retweet_count                                               text  label\n",
       "0                 0  4. can eating garlic help prevent infection wi...      0\n",
       "1               185  french police chief killed himself after  atta...      1\n",
       "2                 1  coronavirus disease (covid-19) advice for the ...      0\n",
       "3               123  ottawa police confirm that there were multiple...      0\n",
       "4                 1  if the primary focus of a government isn't to ...      0\n",
       "...             ...                                                ...    ...\n",
       "1558              0  4. it cannot be transmitted through goods manu...      0\n",
       "1559             83  desperate ted cruz claims planned parenthood s...      1\n",
       "1560            109  \"thoughts and prayers are not enough.\" pres. o...      1\n",
       "1561            177  police have surrounded this building where the...      0\n",
       "1562              4  excellent.  tasha and i just little giggle at ...      0\n",
       "\n",
       "[1563 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train_dfs.csv\",index= False)\n",
    "df_dev.to_csv('dev_dfs.csv',index= False)\n",
    "df_test.to_csv('test_dfs.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Users/yuweng/Desktop/task1/train_dfs.csv')\n",
    "df_dev = pd.read_csv('/Users/yuweng/Desktop/task1/dev_dfs.csv')\n",
    "df_test = pd.read_csv('/Users/yuweng/Desktop/task1/test_dfs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweet_count    False\n",
       "text             False\n",
       "label             True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(axis=0, how='any', inplace=True)\n",
    "df_dev.dropna(axis=0, how='any', inplace=True)\n",
    "df_test.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweet_count    False\n",
       "text             False\n",
       "label            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train_dfs.csv\",index= False)\n",
    "df_dev.to_csv('dev_dfs.csv',index= False)\n",
    "df_test.to_csv('test_dfs.csv',index= False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1efdf83da36bd9fc32e0556c4f59ed9a03a315f474542f72e7cd76a34233f413"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
