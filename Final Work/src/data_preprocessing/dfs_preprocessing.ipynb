{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = { \"in_reply_to_status_id\": None}\n",
    "if a[\"in_reply_to_status_id\"]!=None:\n",
    "    print (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def twitter_feature_selection(json_file):\n",
    "    selected_feature = {}\n",
    "    selected_feature[\"text\"] = json_file[\"text\"]\n",
    "    selected_feature[\"id\"] = json_file[\"id\"]\n",
    "    selected_feature[\"created_at\"] = json_file[\"created_at\"]\n",
    "    if \"referenced_tweets\" in json_file.keys():   \n",
    "        selected_feature[\"referenced_tweets\"] = json_file[\"referenced_tweets\"][0][\"id\"]\n",
    "    selected_feature['retweet_count'] = json_file['public_metrics']['retweet_count']\n",
    "    \n",
    "    # selected_feature[\"hashtag\"] = re.search(r'(\\w+)+', json_file[\"text\"])\n",
    "    return selected_feature\n",
    "\n",
    "def test_feature_selection(json_file):\n",
    "    selected_feature = {}\n",
    "    selected_feature[\"text\"] = json_file[\"text\"]\n",
    "    selected_feature[\"created_at\"] = json_file[\"created_at\"]\n",
    "    if json_file[\"in_reply_to_status_id_str\"]!= None:   \n",
    "        selected_feature[\"referenced_tweets\"] = json_file[\"in_reply_to_status_id_str\"]\n",
    "    selected_feature[\"id\"] = json_file[\"id_str\"]\n",
    "    selected_feature['retweet_count'] = json_file['retweet_count']\n",
    "\n",
    "    return selected_feature\n",
    "\n",
    "def convert_label(label):\n",
    "    if label == \"rumour\":\n",
    "        return 1\n",
    "    elif label == \"nonrumour\":\n",
    "        return 0 \n",
    "def merge_dictionary_list(dict_list):\n",
    "      return {\n",
    "    k: [d.get(k) for d in dict_list if k in d]\n",
    "    for k in set().union(*dict_list)\n",
    "  } \n",
    "def remove_unwanted_keys(dict_list, keys):\n",
    "    for item in dict_list:\n",
    "        item.pop(keys, None)\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getgraph(events):\n",
    "    graph = {}\n",
    "    for e in events:   \n",
    "        graph[e[\"id\"]]=[]\n",
    "\n",
    "    for e in events:\n",
    "        if \"referenced_tweets\" in e.keys():\n",
    "            #str\n",
    "            k = e[\"referenced_tweets\"]\n",
    "            v = e[\"id\"]\n",
    "            if k in graph.keys(): \n",
    "                graph[k].append(e)\n",
    "            else:\n",
    "                graph[k] = [e]\n",
    "    return(graph)\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# time_sequence = []\n",
    "\n",
    "# ## sort dfs as the reply order\n",
    "# ##everynode type is (id,text,time,)\n",
    "# def dfs(visited, graph, node):\n",
    "    \n",
    "#     if node[\"id\"] not in visited:\n",
    "#         time_sequence.append(node)\n",
    "# #         print (node)\n",
    "#         visited.add(node[\"id\"])\n",
    "#         for neighbour in graph[node[\"id\"]]:\n",
    "            \n",
    "#             dfs(visited, graph, neighbour)\n",
    "\n",
    "# eventg= getgraph(testevent)\n",
    "# # # Driver Code\n",
    "# dfs(visited, eventg, sourcetweet)\n",
    "# print (time_sequence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## sort dfs as the reply order\n",
    "##everynode type is (id,text,time,)\n",
    "def dfs(visited, graph, node,time_sequence):\n",
    "    \n",
    "    if node[\"id\"] not in visited:\n",
    "        time_sequence.append(node)\n",
    "#         print (node)\n",
    "        visited.add(node[\"id\"])\n",
    "        for neighbour in graph[node[\"id\"]]:\n",
    "            \n",
    "            dfs(visited, graph, neighbour,time_sequence)\n",
    "    return time_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    if (file_name == 'train' or file_name == 'dev'):\n",
    "        total_json = []\n",
    "        finished_dict = []\n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".data.txt\", \"r\") as f:\n",
    "            file_name_event = f.readlines()      \n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".label.txt\", \"r\") as f:\n",
    "            file_name_label = f.readlines()\n",
    "        for event_str, event_label in zip(file_name_event, file_name_label):\n",
    "            event_list = event_str.rstrip().split(\",\")\n",
    "            event_label = event_label.rstrip().split(\",\")\n",
    "            # print(type(event_label))\n",
    "            event_json = []\n",
    "            path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + event_list[0] + \".json\"\n",
    "            # print(path)\n",
    "            # print(os.path.exists(path))\n",
    "            if os.path.exists(path):\n",
    "                for id in event_list:\n",
    "                    path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + id + \".json\"\n",
    "                    # print(path)\n",
    "                    # print(os.path.exists(path))\n",
    "                    if os.path.exists(path):\n",
    "                        related_tweet = json.load(open(path, \"r\"))\n",
    "                        # print(related_tweet)\n",
    "                        event_json.append(twitter_feature_selection(related_tweet))\n",
    "                \n",
    "                event_json = sorted(event_json, key=lambda x: time.mktime(time.strptime(x[\"created_at\"], '%Y-%m-%dT%H:%M:%S.%fZ')))\n",
    "                sourcetweet = event_json[0]\n",
    "                eventgraph = getgraph(event_json)\n",
    "                visited = set()\n",
    "                sortresult = []\n",
    "                event_json = dfs(visited, eventgraph, sourcetweet,sortresult)\n",
    "                \n",
    "                event_json.append({\"label\":convert_label(''.join(event_label))})\n",
    "                # print(event_json)\n",
    "                merged_dict = merge_dictionary_list(event_json)\n",
    "                # print(merged_dict)\n",
    "                total_json.append(merged_dict)\n",
    "                # print(total_json)\n",
    "                finished_dict = remove_unwanted_keys(total_json, \"created_at\")\n",
    "    elif (file_name == 'test'):\n",
    "        total_json = []\n",
    "        finished_dict = []\n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".data.txt\", \"r\") as f:\n",
    "            file_name_event = f.readlines()      \n",
    "        for event_str in file_name_event:\n",
    "            event_list = event_str.rstrip().split(\",\")\n",
    "            event_json = []\n",
    "            path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + event_list[0] + \".json\"\n",
    "            # print(path)\n",
    "            # print(os.path.exists(path))\n",
    "            if os.path.exists(path):\n",
    "                for id in event_list:\n",
    "                    path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + id + \".json\"\n",
    "                    # print(path)\n",
    "                    # print(os.path.exists(path))\n",
    "                    if os.path.exists(path):\n",
    "                        related_tweet = json.load(open(path, \"r\"))\n",
    "                        # print(related_tweet)\n",
    "                        event_json.append(test_feature_selection(related_tweet))\n",
    "                event_json = sorted(event_json, key=lambda x: time.mktime(time.strptime(x[\"created_at\"], '%a %b %d %H:%M:%S %z %Y')))\n",
    "                # print(event_json)\n",
    "                sourcetweet = event_json[0]\n",
    "                eventgraph = getgraph(event_json)\n",
    "                visited = set()\n",
    "                sortresult = []\n",
    "                event_json = dfs(visited, eventgraph, sourcetweet,sortresult)\n",
    "                \n",
    "                merged_dict = merge_dictionary_list(event_json)\n",
    "                # print(merged_dict)\n",
    "                total_json.append(merged_dict)\n",
    "                # print(total_json)\n",
    "                finished_dict = remove_unwanted_keys(total_json, \"created_at\")\n",
    "    return pd.DataFrame(finished_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Required data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data(\"train\")\n",
    "df_dev = load_data(\"dev\")\n",
    "df_test = load_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(tweet):\n",
    "    r1 = r'https?:/\\/\\S+'\n",
    "    ## remove @people\n",
    "    r2 = r'@[a-zA-Z0-9â€™!\"#$%&\\'()*+,-./:;<=>?@ï¼Œã€‚?â˜…ã€â€¦ï¼Ÿâ€œâ€â€˜â€™ï¼\\\\]+'\n",
    "    ##remove tag\n",
    "    r3 = r'#[a-zA-Z0-9â€™!\"#$%&\\'()*+,-./:;<=>?@ï¼Œã€‚?â˜…ã€â€¦ï¼Ÿâ€œâ€â€˜â€™ï¼\\\\]+'\n",
    "\n",
    "    tweet = tweet.encode('utf-8', 'replace').decode('utf-8')\n",
    "    tweet = re.sub(r1, \"\", tweet)\n",
    "    tweet = re.sub(r2, \"\", tweet)\n",
    "    tweet = re.sub(r3, \"\", str(tweet).lower().strip())\n",
    "    return tweet\n",
    "def give_emoji_free_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\" \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  \n",
    "        u\"\\U0001F1E6-\\U0001F1FF\" \n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the Text together and do the data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹',\n",
       " '@snooki ğŸ˜©ğŸ˜«ğŸ˜©ğŸ˜«ğŸ˜© they just keep finding stuff',\n",
       " '@snooki not fallin for that again',\n",
       " '@snooki nasty!!!!!! ğŸ˜­',\n",
       " '@snooki no reason it should even exist ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜©ğŸ˜©',\n",
       " '@snooki ok',\n",
       " '@snooki NOOOOO ğŸ˜·',\n",
       " '@snooki WTF A PUPPY SIZED SPIDER',\n",
       " 'THIS IS THE SPIDER \\n\\nâ€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€',\n",
       " \"@snooki NO NO NO NO NO NO! That's a horror movie spider!\",\n",
       " \"@snooki lol! I'm the same way with rats....\",\n",
       " '@snooki I thought it was going to be a birdğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­',\n",
       " '@snooki BYEEEEE ğŸ‘‹ğŸ‘‹ğŸ‘‹ğŸ‘‹ğŸ‘‹ğŸ‘‹',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ I JUST CRINGED!!!',\n",
       " '\"@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹\" why would you do this to me?',\n",
       " \"@snooki hence why I'm arachnophobic!!!!\",\n",
       " '@snooki @kltoleyX â¬†ï¸..',\n",
       " '@snooki ğŸ™ˆğŸ˜·ğŸ˜·ğŸ˜·ğŸ˜·',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ @BrookieSnooki10',\n",
       " '@Merritt_Bwn âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹ FUCK THAT SHIT!!',\n",
       " '@snooki Gurllll. I got one for you. Google Camel Spider.',\n",
       " '@snooki ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ OMG ah',\n",
       " '@snooki I hate you...  ğŸ˜±ğŸ˜­ğŸ˜· my curiosity got the best of me',\n",
       " '@snooki hahahaha ğŸ˜‚',\n",
       " '@snooki Fuck u why would u make meğŸ˜­',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ OMGğŸ˜·ğŸ˜·ğŸ˜·ğŸ˜·',\n",
       " '@snooki I noped that so hard! ğŸ™…',\n",
       " '@snooki I threw my phone across the room',\n",
       " '@snooki fuck you omg lol',\n",
       " \"@snooki holy balls!!! That's a BIG ass spider ğŸ˜°ğŸ™ŠğŸ™ˆğŸ™ˆğŸ™ˆ\",\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ğŸ˜©ğŸ˜© http://t.co/o90M9lD37u',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ @jarodjnellly do it',\n",
       " '@snooki wherever it lives just got added to my never want to visit list ğŸ™…',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ @ViancaVMartinez @armandomtz1211',\n",
       " '@snooki ewwwwwwwww!',\n",
       " \"@snooki I'm scared of spiders the size of a dime ğŸ˜­ğŸ˜­ #nogodno #traumatized\",\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ That is a fuck no',\n",
       " '@snooki (Good God Almighty)!',\n",
       " '@snooki you should try Google Camel Spider',\n",
       " '@snooki WHY WHY WWWHHHYYY WOULD U DO THAT TO US???',\n",
       " '@snooki I think I just pissed my pants...',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ WHAT THE ACTUAL ğŸ˜³ğŸ˜³ğŸ˜·',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ if I ever saw one Id just go ahead &amp; die',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€',\n",
       " '@snooki I hate you for that!!!! ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ‘ğŸ‘ğŸ‘ğŸ‘',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ JODA NOOOO TIRE EL CELULARğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜¥',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€@GraceBowlby',\n",
       " '@snooki ğŸ˜·ğŸ˜·',\n",
       " '@snooki all aboard the NOPE train! ğŸš',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜©ğŸ˜©ğŸ˜³',\n",
       " \"@snooki I wanna do it but I'm scared!\",\n",
       " '@snooki EW!!!',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ BYE',\n",
       " '@snooki Omg no',\n",
       " '@snooki thanks for that. ğŸ˜',\n",
       " \"â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ I'd rather die than see that\",\n",
       " '@snooki holy shit. Ew. No no no no no no no no no no.',\n",
       " '@snooki @JessieBFF4EVA annnnnd I now need to change the bed... #HugeAssSpider #PuppySize #AteThePuppy #BedShittage',\n",
       " '@snooki omg I literally stopped breathing!! ğŸ™…ğŸ™…ğŸ™…ğŸ™…',\n",
       " '@snooki ...ewweww....no way',\n",
       " \"@snooki Did you watch the video on YouTube? It's really interesting.\",\n",
       " '@snooki I think you will do great,bc you and Jenny are great in front of the camera..Good luck',\n",
       " '@snooki EW! Why would you do that?! Shame on you ğŸ˜·',\n",
       " '@snooki I kinda do what ever you tell me sooo....... That was not pretty ğŸ˜…',\n",
       " '@snooki omggg I literally have goosebumps from googling that ugly thing!',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ @jahviaskristy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_train['text'] = df_train['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_train['text'] = df_train['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_train['label'] = df_train.apply(lambda x:x[\"label\"][0],axis=1)\n",
    "df_train['label'] = df_train['label'].astype('int')\n",
    "df_train['retweet_count'] = df_train.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "df_dev['text'] = df_dev.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_dev['text'] = df_dev['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_dev['text'] = df_dev['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_dev['label'] = df_dev.apply(lambda x:x[\"label\"][0],axis=1)\n",
    "df_dev['label'] = df_dev['label'].astype('int')\n",
    "df_dev['retweet_count'] = df_dev.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "\n",
    "df_test['text'] = df_test.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_test['text'] = df_test['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_test['text'] = df_test['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_test['retweet_count'] = df_test.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_train.drop(['id','referenced_tweets'],axis=1,inplace=True)\n",
    "df_dev.drop(['id','referenced_tweets'],axis=1,inplace=True)\n",
    "df_test.drop(['id','referenced_tweets'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_html(x))\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_html(x))\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_html(x))\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "df_train.dropna(axis=0, how='any', inplace=True)\n",
    "df_dev.dropna(axis=0, how='any', inplace=True)\n",
    "df_test.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google south american goliath birdeater. helllll noooooo  they just keep finding stuff not fallin for that again nasty!!!!!!  no reason it should even exist  ok nooooo  wtf a puppy sized spiderthis is the spider â€œ google south american goliath birdeater. helllll nooooooâ€ no no no no no no! that\\'s a horror movie spider! lol! i\\'m the same way with rats.... i thought it was going to be a bird byeeeee â€œ google south american goliath birdeater. helllll nooooooâ€ i just cringed!!!\" google south american goliath birdeater. helllll noooooo\" why would you do this to me? hence why i\\'m arachnophobic!!!!  .. â€œ google south american goliath birdeater. helllll nooooooâ€ _bwn  fuck that shit!! gurllll. i got one for you. google camel spider.  omg ah i hate you...   my curiosity got the best of me hahahaha  fuck u why would u make meâ€œ google south american goliath birdeater. helllll nooooooâ€â€œ google south american goliath birdeater. helllll nooooooâ€ omg i noped that so hard!  i threw my phone across the room fuck you omg lol holy balls!!! that\\'s a big ass spider â€œ google south american goliath birdeater. helllll nooooooâ€  google south american goliath birdeater. helllll nooooooâ€  do it wherever it lives just got added to my never want to visit list â€œ google south american goliath birdeater. helllll nooooooâ€   ewwwwwwwww! i\\'m scared of spiders the size of a dime    google south american goliath birdeater. helllll nooooooâ€ that is a fuck no (good god almighty)! you should try google camel spider why why wwwhhhyyy would u do that to us??? i think i just pissed my pants...â€œ google south american goliath birdeater. helllll nooooooâ€ what the actual â€œ google south american goliath birdeater. helllll nooooooâ€ if i ever saw one id just go ahead &amp; dieâ€œ google south american goliath birdeater. helllll nooooooâ€ i hate you for that!!!! â€œ google south american goliath birdeater. helllll nooooooâ€ joda noooo tire el celularâ€œ google south american goliath birdeater. helllll nooooooâ€  all aboard the nope train! â€œ google south american goliath birdeater. helllll nooooooâ€  i wanna do it but i\\'m scared! ew!!!â€œ google south american goliath birdeater. helllll nooooooâ€ bye omg no thanks for that. â€œ google south american goliath birdeater. helllll nooooooâ€ i\\'d rather die than see that holy shit. ew. no no no no no no no no no no.  annnnnd i now need to change the bed...     omg i literally stopped breathing!!  ...ewweww....no way did you watch the video on youtube? it\\'s really interesting. i think you will do great,bc you and jenny are great in front of the camera..good luck ew! why would you do that?! shame on you  i kinda do what ever you tell me sooo....... that was not pretty  omggg i literally have goosebumps from googling that ugly thing!â€œ google south american goliath birdeater. helllll nooooooâ€'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweet_count    False\n",
       "text             False\n",
       "label            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4. can eating garlic help prevent infection wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185</td>\n",
       "      <td>french police chief killed himself after  atta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus disease (covid-19) advice for the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>ottawa police confirm that there were multiple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>if the primary focus of a government isn't to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0</td>\n",
       "      <td>4. it cannot be transmitted through goods manu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>83</td>\n",
       "      <td>desperate ted cruz claims planned parenthood s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>109</td>\n",
       "      <td>\"thoughts and prayers are not enough.\" pres. o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>177</td>\n",
       "      <td>police have surrounded this building where the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>4</td>\n",
       "      <td>excellent.  tasha and i just little giggle at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1563 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      retweet_count                                               text  label\n",
       "0                 0  4. can eating garlic help prevent infection wi...      0\n",
       "1               185  french police chief killed himself after  atta...      1\n",
       "2                 1  coronavirus disease (covid-19) advice for the ...      0\n",
       "3               123  ottawa police confirm that there were multiple...      0\n",
       "4                 1  if the primary focus of a government isn't to ...      0\n",
       "...             ...                                                ...    ...\n",
       "1558              0  4. it cannot be transmitted through goods manu...      0\n",
       "1559             83  desperate ted cruz claims planned parenthood s...      1\n",
       "1560            109  \"thoughts and prayers are not enough.\" pres. o...      1\n",
       "1561            177  police have surrounded this building where the...      0\n",
       "1562              4  excellent.  tasha and i just little giggle at ...      0\n",
       "\n",
       "[1563 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train_dfs.csv\",index= False)\n",
    "df_dev.to_csv('dev_dfs.csv',index= False)\n",
    "df_test.to_csv('test_dfs.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Users/yuweng/Desktop/task1/train_dfs.csv')\n",
    "df_dev = pd.read_csv('/Users/yuweng/Desktop/task1/dev_dfs.csv')\n",
    "df_test = pd.read_csv('/Users/yuweng/Desktop/task1/test_dfs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweet_count    False\n",
       "text             False\n",
       "label             True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(axis=0, how='any', inplace=True)\n",
    "df_dev.dropna(axis=0, how='any', inplace=True)\n",
    "df_test.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweet_count    False\n",
       "text             False\n",
       "label            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train_dfs.csv\",index= False)\n",
    "df_dev.to_csv('dev_dfs.csv',index= False)\n",
    "df_test.to_csv('test_dfs.csv',index= False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1efdf83da36bd9fc32e0556c4f59ed9a03a315f474542f72e7cd76a34233f413"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
