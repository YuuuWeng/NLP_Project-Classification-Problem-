{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def twitter_feature_selection(json_file):\n",
    "    selected_feature = {}\n",
    "    selected_feature[\"text\"] = json_file[\"text\"]\n",
    "    # selected_feature[\"id\"] = json_file[\"id\"]\n",
    "    selected_feature[\"created_at\"] = json_file[\"created_at\"]\n",
    "    # selected_feature['like_count'] = json_file['public_metrics']['like_count']\n",
    "    selected_feature['retweet_count'] = json_file['public_metrics']['retweet_count']\n",
    "\n",
    "    # selected_feature[\"hashtag\"] = re.search(r'(\\w+)+', json_file[\"text\"])\n",
    "    return selected_feature\n",
    "\n",
    "def test_feature_selection(json_file):\n",
    "    selected_feature = {}\n",
    "    selected_feature[\"text\"] = json_file[\"text\"]\n",
    "    selected_feature[\"created_at\"] = json_file[\"created_at\"]\n",
    "    selected_feature['retweet_count'] = json_file['retweet_count']\n",
    "\n",
    "    # selected_feature[\"id\"] = json_file[\"id\"]\n",
    "    return selected_feature\n",
    "\n",
    "def convert_label(label):\n",
    "    if label == \"rumour\":\n",
    "        return 1\n",
    "    elif label == \"nonrumour\":\n",
    "        return 0 \n",
    "def merge_dictionary_list(dict_list):\n",
    "      return {\n",
    "    k: [d.get(k) for d in dict_list if k in d]\n",
    "    for k in set().union(*dict_list)\n",
    "  } \n",
    "def remove_unwanted_keys(dict_list, keys):\n",
    "    for item in dict_list:\n",
    "        item.pop(keys, None)\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    if (file_name == 'train' or file_name == 'dev'):\n",
    "        total_json = []\n",
    "        finished_dict = []\n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".data.txt\", \"r\") as f:\n",
    "            file_name_event = f.readlines()      \n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".label.txt\", \"r\") as f:\n",
    "            file_name_label = f.readlines()\n",
    "        for event_str, event_label in zip(file_name_event, file_name_label):\n",
    "            event_list = event_str.rstrip().split(\",\")\n",
    "            event_label = event_label.rstrip().split(\",\")\n",
    "            event_json = []\n",
    "            path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + event_list[0] + \".json\"\n",
    "            # print(path)\n",
    "            # print(os.path.exists(path))\n",
    "            if os.path.exists(path):\n",
    "                for id in event_list:\n",
    "                    path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + id + \".json\"\n",
    "                    # print(path)\n",
    "                    # print(os.path.exists(path))\n",
    "                    if os.path.exists(path):\n",
    "                        related_tweet = json.load(open(path, \"r\"))\n",
    "                        # print(related_tweet)\n",
    "                        event_json.append(twitter_feature_selection(related_tweet))\n",
    "                        # print(event_json)\n",
    "                event_json = sorted(event_json, key=lambda x: time.mktime(time.strptime(x[\"created_at\"], '%Y-%m-%dT%H:%M:%S.%fZ')))\n",
    "                event_json.append({\"label\":convert_label(''.join(event_label))})\n",
    "                # print(event_json)\n",
    "                merged_dict = merge_dictionary_list(event_json)\n",
    "                # print(merged_dict)\n",
    "                total_json.append(merged_dict)\n",
    "                # print(total_json)\n",
    "                finished_dict = remove_unwanted_keys(total_json, \"created_at\")\n",
    "    elif (file_name == 'test'):\n",
    "        total_json = []\n",
    "        finished_dict = []\n",
    "        with open(\"/Users/yuweng/Desktop/task1/\" + file_name +\".data.txt\", \"r\") as f:\n",
    "            file_name_event = f.readlines()      \n",
    "        for event_str in file_name_event:\n",
    "            event_list = event_str.rstrip().split(\",\")\n",
    "            event_json = []\n",
    "            path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + event_list[0] + \".json\"\n",
    "            # print(path)\n",
    "            # print(os.path.exists(path))\n",
    "            if os.path.exists(path):\n",
    "                for id in event_list:\n",
    "                    path = \"/Users/yuweng/Desktop/task1/\" + file_name +'/' + id + \".json\"\n",
    "                    # print(path)\n",
    "                    # print(os.path.exists(path))\n",
    "                    if os.path.exists(path):\n",
    "                        related_tweet = json.load(open(path, \"r\"))\n",
    "                        # print(related_tweet)\n",
    "                        event_json.append(test_feature_selection(related_tweet))\n",
    "                event_json = sorted(event_json, key=lambda x: time.mktime(time.strptime(x[\"created_at\"], '%a %b %d %H:%M:%S %z %Y')))\n",
    "                # print(event_json)\n",
    "                merged_dict = merge_dictionary_list(event_json)\n",
    "                # print(merged_dict)\n",
    "                total_json.append(merged_dict)\n",
    "                # print(total_json)\n",
    "                finished_dict = remove_unwanted_keys(total_json, \"created_at\")\n",
    "    return pd.DataFrame(finished_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Required data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data(\"train\")\n",
    "df_dev = load_data(\"dev\")\n",
    "df_test = load_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(tweet):\n",
    "    r1 = r'https?:/\\/\\S+'\n",
    "    ## remove @people\n",
    "    r2 = r'@[a-zA-Z0-9â€™!\"#$%&\\'()*+,-./:;<=>?@ï¼Œã€‚?â˜…ã€â€¦ï¼Ÿâ€œâ€â€˜â€™ï¼\\\\]+'\n",
    "    ##remove tag\n",
    "    r3 = r'#[a-zA-Z0-9â€™!\"#$%&\\'()*+,-./:;<=>?@ï¼Œã€‚?â˜…ã€â€¦ï¼Ÿâ€œâ€â€˜â€™ï¼\\\\]+'\n",
    "\n",
    "    tweet = tweet.encode('utf-8', 'replace').decode('utf-8')\n",
    "    tweet = re.sub(r1, \"\", tweet)\n",
    "    tweet = re.sub(r2, \"\", tweet)\n",
    "    tweet = re.sub(r3, \"\", str(tweet).lower().strip())\n",
    "    return tweet\n",
    "def give_emoji_free_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\" \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  \n",
    "        u\"\\U0001F1E6-\\U0001F1FF\" \n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the Text together and do the data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹',\n",
       " '@snooki ğŸ˜©ğŸ˜«ğŸ˜©ğŸ˜«ğŸ˜© they just keep finding stuff',\n",
       " '@snooki not fallin for that again',\n",
       " '@snooki nasty!!!!!! ğŸ˜­',\n",
       " '@snooki no reason it should even exist ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜©ğŸ˜©',\n",
       " '@snooki ok',\n",
       " '@snooki NOOOOO ğŸ˜·',\n",
       " '@snooki WTF A PUPPY SIZED SPIDER',\n",
       " 'THIS IS THE SPIDER \\n\\nâ€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€',\n",
       " \"@snooki NO NO NO NO NO NO! That's a horror movie spider!\",\n",
       " \"@snooki lol! I'm the same way with rats....\",\n",
       " '@snooki I thought it was going to be a birdğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­',\n",
       " '@snooki BYEEEEE ğŸ‘‹ğŸ‘‹ğŸ‘‹ğŸ‘‹ğŸ‘‹ğŸ‘‹',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ I JUST CRINGED!!!',\n",
       " '\"@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹\" why would you do this to me?',\n",
       " \"@snooki hence why I'm arachnophobic!!!!\",\n",
       " '@snooki @kltoleyX â¬†ï¸..',\n",
       " '@snooki ğŸ™ˆğŸ˜·ğŸ˜·ğŸ˜·ğŸ˜·',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ @BrookieSnooki10',\n",
       " '@snooki Gurllll. I got one for you. Google Camel Spider.',\n",
       " '@snooki ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ OMG ah',\n",
       " '@snooki I hate you...  ğŸ˜±ğŸ˜­ğŸ˜· my curiosity got the best of me',\n",
       " '@snooki hahahaha ğŸ˜‚',\n",
       " '@snooki Fuck u why would u make meğŸ˜­',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ OMGğŸ˜·ğŸ˜·ğŸ˜·ğŸ˜·',\n",
       " '@snooki I noped that so hard! ğŸ™…',\n",
       " '@snooki I threw my phone across the room',\n",
       " '@snooki fuck you omg lol',\n",
       " \"@snooki holy balls!!! That's a BIG ass spider ğŸ˜°ğŸ™ŠğŸ™ˆğŸ™ˆğŸ™ˆ\",\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ğŸ˜©ğŸ˜© http://t.co/o90M9lD37u',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ @jarodjnellly do it',\n",
       " '@snooki wherever it lives just got added to my never want to visit list ğŸ™…',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ @ViancaVMartinez @armandomtz1211',\n",
       " '@snooki ewwwwwwwww!',\n",
       " \"@snooki I'm scared of spiders the size of a dime ğŸ˜­ğŸ˜­ #nogodno #traumatized\",\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ That is a fuck no',\n",
       " '@snooki (Good God Almighty)!',\n",
       " '@snooki you should try Google Camel Spider',\n",
       " '@snooki WHY WHY WWWHHHYYY WOULD U DO THAT TO US???',\n",
       " '@snooki I think I just pissed my pants...',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ WHAT THE ACTUAL ğŸ˜³ğŸ˜³ğŸ˜·',\n",
       " '@Merritt_Bwn âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹ FUCK THAT SHIT!!',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ if I ever saw one Id just go ahead &amp; die',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€',\n",
       " '@snooki I hate you for that!!!! ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ‘ğŸ‘ğŸ‘ğŸ‘',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ JODA NOOOO TIRE EL CELULARğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜¥',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€@GraceBowlby',\n",
       " '@snooki ğŸ˜·ğŸ˜·',\n",
       " '@snooki all aboard the NOPE train! ğŸš',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜©ğŸ˜©ğŸ˜³',\n",
       " \"@snooki I wanna do it but I'm scared!\",\n",
       " '@snooki EW!!!',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ BYE',\n",
       " '@snooki Omg no',\n",
       " '@snooki thanks for that. ğŸ˜',\n",
       " \"â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ I'd rather die than see that\",\n",
       " '@snooki holy shit. Ew. No no no no no no no no no no.',\n",
       " '@snooki @JessieBFF4EVA annnnnd I now need to change the bed... #HugeAssSpider #PuppySize #AteThePuppy #BedShittage',\n",
       " '@snooki omg I literally stopped breathing!! ğŸ™…ğŸ™…ğŸ™…ğŸ™…',\n",
       " '@snooki ...ewweww....no way',\n",
       " \"@snooki Did you watch the video on YouTube? It's really interesting.\",\n",
       " '@snooki I think you will do great,bc you and Jenny are great in front of the camera..Good luck',\n",
       " '@snooki EW! Why would you do that?! Shame on you ğŸ˜·',\n",
       " '@snooki I kinda do what ever you tell me sooo....... That was not pretty ğŸ˜…',\n",
       " '@snooki omggg I literally have goosebumps from googling that ugly thing!',\n",
       " '@lulu2038 @snooki @BradleyStyleNYC I just had a tick crawling on my shirt just almost died ğŸ˜³ğŸ˜–ğŸ˜±ğŸ˜µ',\n",
       " 'â€œ@snooki: Google South American Goliath Birdeater. HELLLLL NOOOOOOâœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹âœ‹â€ @jahviaskristy']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_train['text'] = df_train['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_train['text'] = df_train['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_train['label'] = df_train.apply(lambda x:x[\"label\"][0],axis=1)\n",
    "df_train['label'] = df_train['label'].astype('int')\n",
    "df_train['retweet_count'] = df_train.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "df_dev['text'] = df_dev.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_dev['text'] = df_dev['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_dev['text'] = df_dev['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_dev['label'] = df_dev.apply(lambda x:x[\"label\"][0],axis=1)\n",
    "df_dev['label'] = df_dev['label'].astype('int')\n",
    "df_dev['retweet_count'] = df_dev.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "df_test['text'] = df_test.apply(lambda x:text_preprocessing(''.join(x[\"text\"])),axis=1)\n",
    "df_test['text'] = df_test['text'].apply(lambda x:x.lower().replace('\\n',''))\n",
    "df_test['text'] = df_test['text'].apply(lambda x:give_emoji_free_text(x))\n",
    "df_test['retweet_count'] = df_test.apply(lambda x:sum(x[\"retweet_count\"]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_html(x))\n",
    "# df_train['text'] = df_train['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_html(x))\n",
    "# df_dev['text'] = df_dev['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_URL(x))\n",
    "\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_html(x))\n",
    "# df_test['text'] = df_test['text'].apply(lambda x:remove_punct(x))\n",
    "\n",
    "# df_train = df_train.dropna(axis=0, how='any')\n",
    "# df_dev = df_dev.dropna(axis=0, how='any')\n",
    "# df_test = df_test.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google south american goliath birdeater. helllll noooooo  they just keep finding stuff not fallin for that again nasty!!!!!!  no reason it should even exist  ok nooooo  wtf a puppy sized spiderthis is the spider â€œ google south american goliath birdeater. helllll nooooooâ€ no no no no no no! that\\'s a horror movie spider! lol! i\\'m the same way with rats.... i thought it was going to be a bird byeeeee â€œ google south american goliath birdeater. helllll nooooooâ€ i just cringed!!!\" google south american goliath birdeater. helllll noooooo\" why would you do this to me? hence why i\\'m arachnophobic!!!!  .. â€œ google south american goliath birdeater. helllll nooooooâ€  gurllll. i got one for you. google camel spider.  omg ah i hate you...   my curiosity got the best of me hahahaha  fuck u why would u make meâ€œ google south american goliath birdeater. helllll nooooooâ€â€œ google south american goliath birdeater. helllll nooooooâ€ omg i noped that so hard!  i threw my phone across the room fuck you omg lol holy balls!!! that\\'s a big ass spider â€œ google south american goliath birdeater. helllll nooooooâ€  google south american goliath birdeater. helllll nooooooâ€  do it wherever it lives just got added to my never want to visit list â€œ google south american goliath birdeater. helllll nooooooâ€   ewwwwwwwww! i\\'m scared of spiders the size of a dime    google south american goliath birdeater. helllll nooooooâ€ that is a fuck no (good god almighty)! you should try google camel spider why why wwwhhhyyy would u do that to us??? i think i just pissed my pants...â€œ google south american goliath birdeater. helllll nooooooâ€ what the actual _bwn  fuck that shit!!â€œ google south american goliath birdeater. helllll nooooooâ€ if i ever saw one id just go ahead &amp; dieâ€œ google south american goliath birdeater. helllll nooooooâ€ i hate you for that!!!! â€œ google south american goliath birdeater. helllll nooooooâ€ joda noooo tire el celularâ€œ google south american goliath birdeater. helllll nooooooâ€  all aboard the nope train! â€œ google south american goliath birdeater. helllll nooooooâ€  i wanna do it but i\\'m scared! ew!!!â€œ google south american goliath birdeater. helllll nooooooâ€ bye omg no thanks for that. â€œ google south american goliath birdeater. helllll nooooooâ€ i\\'d rather die than see that holy shit. ew. no no no no no no no no no no.  annnnnd i now need to change the bed...     omg i literally stopped breathing!!  ...ewweww....no way did you watch the video on youtube? it\\'s really interesting. i think you will do great,bc you and jenny are great in front of the camera..good luck ew! why would you do that?! shame on you  i kinda do what ever you tell me sooo....... that was not pretty  omggg i literally have goosebumps from googling that ugly thing!   i just had a tick crawling on my shirt just almost died â€œ google south american goliath birdeater. helllll nooooooâ€'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4. can eating garlic help prevent infection wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>french police chief killed himself after  atta...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>coronavirus disease (covid-19) advice for the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ottawa police confirm that there were multiple...</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>if the primary focus of a government isn't to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0</td>\n",
       "      <td>4. it cannot be transmitted through goods manu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>1</td>\n",
       "      <td>desperate ted cruz claims planned parenthood s...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>1</td>\n",
       "      <td>\"thoughts and prayers are not enough.\" pres. o...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>0</td>\n",
       "      <td>police have surrounded this building where the...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>0</td>\n",
       "      <td>excellent.  tasha and i just little giggle at ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1563 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  retweet_count\n",
       "0         0  4. can eating garlic help prevent infection wi...              0\n",
       "1         1  french police chief killed himself after  atta...            185\n",
       "2         0  coronavirus disease (covid-19) advice for the ...              1\n",
       "3         0  ottawa police confirm that there were multiple...            123\n",
       "4         0  if the primary focus of a government isn't to ...              1\n",
       "...     ...                                                ...            ...\n",
       "1558      0  4. it cannot be transmitted through goods manu...              1\n",
       "1559      1  desperate ted cruz claims planned parenthood s...             83\n",
       "1560      1  \"thoughts and prayers are not enough.\" pres. o...            109\n",
       "1561      0  police have surrounded this building where the...            177\n",
       "1562      0  excellent.  tasha and i just little giggle at ...              4\n",
       "\n",
       "[1563 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.csv', index=False)\n",
    "df_dev.to_csv('dev.csv', index = False)\n",
    "df_test.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Users/yuweng/Desktop/task1/train.csv')\n",
    "df_dev = pd.read_csv('/Users/yuweng/Desktop/task1/dev.csv')\n",
    "df_test = pd.read_csv('/Users/yuweng/Desktop/task1/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label            False\n",
       "text             False\n",
       "retweet_count     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(axis=0, how='any', inplace=True)\n",
    "df_dev.dropna(axis=0, how='any', inplace=True)\n",
    "df_test.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train.csv\",index= False)\n",
    "df_dev.to_csv('dev.csv',index= False)\n",
    "df_test.to_csv('test.csv',index= False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1efdf83da36bd9fc32e0556c4f59ed9a03a315f474542f72e7cd76a34233f413"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
